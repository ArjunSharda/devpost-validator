import re
from typing import List, Dict, Any, Tuple, Set
import os
import tempfile
import shutil
from pathlib import Path
import json
import statistics
from collections import Counter


class AIDetector:
    def __init__(self):
        self.ai_patterns = [
            (r"#\s*Generated by (ChatGPT|GPT|Claude|Bard|Gemini|Copilot|AI|LLM)", "AI generation attribution", "high"),
            (r"//\s*Generated by (ChatGPT|GPT|Claude|Bard|Gemini|Copilot|AI|LLM)", "AI generation attribution", "high"),
            (
            r"/\*\s*Generated by (ChatGPT|GPT|Claude|Bard|Gemini|Copilot|AI|LLM)", "AI generation attribution", "high"),
            (r"<!--\s*Generated by (ChatGPT|GPT|Claude|Bard|Gemini|Copilot|AI|LLM)", "AI generation attribution",
             "high"),
            (
            r"'''\s*Generated by (ChatGPT|GPT|Claude|Bard|Gemini|Copilot|AI|LLM)", "AI generation attribution", "high"),
            (
            r'"""\s*Generated by (ChatGPT|GPT|Claude|Bard|Gemini|Copilot|AI|LLM)', "AI generation attribution", "high"),
            (r"#\s*\.\.\.existing code\.\.\.", "Placeholder comment", "medium"),
            (r"//\s*\.\.\.existing code\.\.\.", "Placeholder comment", "medium"),
            (r"//\s*your code here", "Code stub comment", "medium"),
            (r"//\s*This is a mock implementation", "Mock implementation comment", "medium"),
            (r"#\s*TODO: (Implement|Complete|Fix|Update|Add|Remove)", "TODO placeholder", "low"),
            (r"//\s*TODO: (Implement|Complete|Fix|Update|Add|Remove)", "TODO placeholder", "low"),
            (r"#\s*FIXME:", "FIXME placeholder", "low"),
            (r"//\s*FIXME:", "FIXME placeholder", "low"),
            (r"As an AI (language model|assistant)", "AI self-reference", "high"),
            (r"As a (language model|AI assistant)", "AI self-reference", "high"),
            (r"I'm an AI (language model|assistant)", "AI self-reference", "high"),
            (r"I do not have (personal|subjective) (opinions|feelings|thoughts)", "AI hedging statement", "high"),
            (r"As of my last (update|training|knowledge cutoff)", "AI knowledge limitation statement", "high"),
            (r"I don't have access to (real-time|current) information", "AI knowledge limitation statement", "high"),
            (r"Here's a (simple|basic|sample|example) implementation", "LLM explanatory phrasing", "medium"),
            (r"Let me (explain|walk you through|break down) (this|how this works)", "LLM explanatory phrasing",
             "medium"),
            (r"First, let's (start|begin) by", "LLM sequential explanation", "medium"),
            (r"Next, we (need to|should|will|can)", "LLM sequential explanation", "medium"),
            (r"Finally, (we|let's)", "LLM sequential explanation", "medium"),
            (r"Let me know if you have any questions", "LLM engagement prompt", "high"),
            (r"I hope this (helps|is helpful|works for you)", "LLM closing statement", "high"),
            (r"Feel free to (modify|adjust|customize) (this|it|as needed)", "LLM customization prompt", "high"),
            (r"For more (information|details), you can refer to", "LLM reference pattern", "medium"),
            (r"This is just a (basic|simple) (example|implementation)", "LLM scope limitation", "medium"),
            (r"function\d+|class\d+", "Systematically numbered entities", "medium"),
            (r"step\s*\d+:", "Sequential step pattern", "medium"),
            (r"option\s*\d+:", "Enumerated options pattern", "medium")
        ]

        self.code_structure_patterns = [
            (r"(def\s+\w+\([^)]*\):(?:\s*\w+\s*=\s*[^;]+;?){3,}){3,}", "Highly repetitive code blocks", "medium"),
            (r"(class\s+\w+\s*{[^}]*}){3,}", "Repetitive class definitions", "medium"),
            (r"(function\s+\w+\s*\([^)]*\)\s*{[^}]*}){3,}", "Repetitive function definitions", "medium"),
            (r"(\{\s*key:\s*['\"]\w+['\"],\s*value:\s*\w+\s*\},?\s*){5,}", "Repetitive data structure", "medium"),
            (r"(<div\s+className=['\"][^'\"]+['\"]\s*>\s*<[^>]+>[^<]*<\/[^>]+>\s*<\/div>\s*){5,}",
             "Repetitive React component structure", "medium"),
            (r"(\s+if\s+\(\w+\s*===\s*['\"][^'\"]+['\"]\)\s*{\s*return\s+[^;]+;\s*}\s*){5,}",
             "Repetitive conditional returns", "medium")
        ]

        self.unnatural_patterns = [
            (r"(?:#[^\n]*\n){5,}", "Excessive sequential comments", "medium"),
            (r'"""\s*\w+\s*\n\s*Parameters:\s*\n\s*-+\s*\n.*\n\s*Returns:\s*\n\s*-+\s*\n.*\n\s*"""\s*',
             "Formulaic docstring", "medium"),
            (r"@param\s+\w+\s+[A-Z].*\n\s*@return\s+[A-Z]", "Formulaic JavaDoc comments", "medium"),
            (r"(\/\/\s*[A-Z][^\\n]{30,}[\.\!]\s*\n){3,}", "Excessive detailed comments", "medium"),
            (r"(\/\*\*\s*\n\s*\*\s*[A-Z].*\n\s*\*\/\s*\n){3,}", "Repetitive block comments", "medium")
        ]

        self.comprehensive_patterns = [
            (
            r"\/\/\s*This (code|function|class|program|script) (was|is) (created|generated|written|made) (by|with|using) (ChatGPT|GPT-\d|Claude|Bard|Gemini|Copilot|GPT|LLM|AI)",
            "Explicit AI attribution", "critical"),
            (r"\/\/\s*AI-generated (code|solution|implementation)", "Explicit AI attribution", "critical"),
            (r"\{\/\* eslint-disable \*\/\}", "Disabled linting", "low"),
            (r"\/\/\s*eslint-disable-next-line", "Disabled linting for line", "low"),
            (r"# noqa", "Ignored Python linting", "low"),
            (r"#\s*pep8: disable", "Disabled Python style guide", "low"),
            (r"def solve_problem\(", "Generic problem-solving function", "low"),
            (r"def solution\(", "Generic solution function", "low"),
            (r"function solution\(", "Generic solution function JS", "low"),
            (r"const solution = \(", "Generic solution function JS arrow", "low")
        ]

        self.language_patterns = {
            "python": [
                (r"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt",
                 "Standard data science imports", "low"),
                (r"from sklearn\.", "Machine learning imports", "low"),
                (r"def __init__\(self, \*args, \*\*kwargs\):", "Generic constructor with unused args", "medium"),
                (r"if __name__ == ['\"]__main__['\"]:", "Standard main block", "low"),
                (r"raise NotImplementedError\(['\"][^'\"]*['\"]\)", "NotImplementedError placeholder", "medium")
            ],
            "javascript": [
                (r"import React, \{ useState, useEffect \} from 'react';", "Standard React imports", "low"),
                (r"const \[\w+, set\w+\] = useState\([^)]*\);", "React useState pattern", "low"),
                (r"useEffect\(\(\) => \{[^}]*\}, \[\]\);", "React useEffect pattern", "low"),
                (r"export default function \w+\([^)]*\) \{", "React function component", "low"),
                (r"const \w+ = \([^)]*\) => \{", "Arrow function pattern", "low"),
                (r"console\.log\(['\"][^'\"]*['\"]\);", "Debug logging", "low")
            ],
            "java": [
                (r"public static void main\(String\[\] args\) \{", "Standard main method", "low"),
                (r"System\.out\.println\(['\"][^'\"]*['\"]\);", "Standard output", "low"),
                (r"@Override\s+public \w+ \w+\([^)]*\) \{", "Override method pattern", "low"),
                (r"try \{[^}]*\} catch \(Exception e\) \{[^}]*\}", "Generic exception catch", "medium")
            ]
        }

    def analyze_code(self, code_content: str, filename: str) -> List[Dict[str, Any]]:
        findings = []

        all_patterns = (
                self.ai_patterns +
                self.code_structure_patterns +
                self.unnatural_patterns +
                self.comprehensive_patterns
        )

        extension = Path(filename).suffix.lower()
        if extension == '.py':
            all_patterns += self.language_patterns.get("python", [])
        elif extension in ['.js', '.jsx', '.ts', '.tsx']:
            all_patterns += self.language_patterns.get("javascript", [])
        elif extension == '.java':
            all_patterns += self.language_patterns.get("java", [])

        for pattern, description, confidence in all_patterns:
            try:
                matches = re.finditer(pattern, code_content, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    line_number = code_content[:match.start()].count('\n') + 1
                    findings.append({
                        "file": filename,
                        "line": line_number,
                        "pattern": pattern,
                        "description": description,
                        "match": match.group(0)[:50] + "..." if len(match.group(0)) > 50 else match.group(0),
                        "confidence": confidence
                    })
            except re.error:
                pass

        semantic_findings = self._analyze_semantic_patterns(code_content, filename)
        findings.extend(semantic_findings)

        return findings

    def _analyze_semantic_patterns(self, code_content: str, filename: str) -> List[Dict[str, Any]]:
        findings = []

        line_count = code_content.count('\n') + 1
        if line_count > 0:
            comment_lines = 0
            for line in code_content.split('\n'):
                line = line.strip()
                if line.startswith('#') or line.startswith('//') or line.startswith('*'):
                    comment_lines += 1

            comment_ratio = comment_lines / line_count
            if comment_ratio > 0.4:
                findings.append({
                    "file": filename,
                    "line": 1,
                    "pattern": "high_comment_ratio",
                    "description": "Unusually high comment to code ratio",
                    "match": f"Comment ratio: {comment_ratio:.2f}",
                    "confidence": "medium"
                })

        variable_pattern = re.compile(r'(?:var|let|const)\s+(\w+)\s*=|(\w+)\s*=|def\s+(\w+)|class\s+(\w+)')
        variables = []
        for match in variable_pattern.finditer(code_content):
            var_name = next((g for g in match.groups() if g), None)
            if var_name and not var_name.startswith('_'):
                variables.append(var_name)

        if len(variables) > 5:
            var_counter = Counter(variables)
            duplication_ratio = 1.0 - (len(var_counter) / len(variables))
            if duplication_ratio > 0.3:
                findings.append({
                    "file": filename,
                    "line": 1,
                    "pattern": "repetitive_variables",
                    "description": "Repetitive variable names",
                    "match": f"Duplication ratio: {duplication_ratio:.2f}",
                    "confidence": "medium"
                })

            var_lengths = [len(v) for v in var_counter.keys()]
            if var_lengths and statistics.mean(var_lengths) < 3:
                findings.append({
                    "file": filename,
                    "line": 1,
                    "pattern": "short_variable_names",
                    "description": "Unusually short variable names",
                    "match": f"Average var length: {statistics.mean(var_lengths):.2f}",
                    "confidence": "low"
                })

        return findings

    def analyze_directory(self, directory_path: str) -> List[Dict[str, Any]]:
        all_findings = []

        for root, _, files in os.walk(directory_path):
            if any(excluded in root for excluded in
                   ['.git', 'node_modules', '__pycache__', 'venv', 'env', 'dist', 'build']):
                continue

            for file in files:
                if file.startswith('.'):
                    continue

                file_path = os.path.join(root, file)

                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()

                    rel_path = os.path.relpath(file_path, directory_path)
                    findings = self.analyze_code(content, rel_path)
                    all_findings.extend(findings)

                except Exception:
                    pass

        return all_findings

    def analyze_repo_content(self, local_path: str) -> Tuple[List[Dict[str, Any]], float]:
        findings = self.analyze_directory(local_path)

        if not findings:
            ai_score = 0.0
        else:
            total_files = self._count_files(local_path)

            critical_count = sum(1 for f in findings if f.get("confidence") == "critical")
            high_count = sum(1 for f in findings if f.get("confidence") == "high")
            medium_count = sum(1 for f in findings if f.get("confidence") == "medium")
            low_count = sum(1 for f in findings if f.get("confidence") == "low")

            unique_files_with_findings = len(set(f.get("file", "") for f in findings))
            file_coverage_ratio = unique_files_with_findings / total_files if total_files > 0 else 0

            base_score = (
                                     critical_count * 0.35 + high_count * 0.2 + medium_count * 0.08 + low_count * 0.03) / total_files if total_files > 0 else 0
            ai_score = min(0.95, base_score * (1 + file_coverage_ratio))

            if critical_count > 0:
                ai_score = max(ai_score, 0.8)

        return findings, ai_score

    def _count_files(self, directory_path: str) -> int:
        file_count = 0
        for root, _, files in os.walk(directory_path):
            if any(excluded in root for excluded in
                   ['.git', 'node_modules', '__pycache__', 'venv', 'env', 'dist', 'build']):
                continue

            for file in files:
                if not file.startswith('.') and not file.endswith(
                        ('.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.pdf', '.zip', '.tar.gz')):
                    file_count += 1

        return max(1, file_count)

    def assess_ai_confidence(self, findings: List[Dict[str, Any]]) -> Dict[str, Any]:
        if not findings:
            return {"overall": "low", "score": 0.0, "indicators": []}

        critical_count = sum(1 for f in findings if f.get("confidence") == "critical")
        high_count = sum(1 for f in findings if f.get("confidence") == "high")
        medium_count = sum(1 for f in findings if f.get("confidence") == "medium")
        low_count = sum(1 for f in findings if f.get("confidence") == "low")

        weighted_score = (critical_count * 1.0 + high_count * 0.7 + medium_count * 0.3 + low_count * 0.1) / len(
            findings)

        confidence = "low"
        if weighted_score > 0.7 or critical_count > 0:
            confidence = "high"
        elif weighted_score > 0.3:
            confidence = "medium"

        top_indicators = sorted(
            [f for f in findings if f.get("confidence") in ["critical", "high"]],
            key=lambda x: {"critical": 0, "high": 1, "medium": 2, "low": 3}[x.get("confidence", "low")]
        )[:5]

        indicator_summary = [
            {"pattern": i["description"], "confidence": i["confidence"], "file": i["file"], "line": i["line"]}
            for i in top_indicators
        ]

        return {
            "overall": confidence,
            "score": weighted_score,
            "indicators": indicator_summary,
            "counts": {
                "critical": critical_count,
                "high": high_count,
                "medium": medium_count,
                "low": low_count,
                "total": len(findings)
            }
        }